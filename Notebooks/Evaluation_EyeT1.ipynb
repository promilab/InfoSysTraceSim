{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "#from scipy.stats import mannwhitneyu\n",
    "#import collections\n",
    "#from statistics import mean\n",
    "#from statistics import median\n",
    "from collections import defaultdict\n",
    "from Levenshtein import distance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn import metrics\n",
    "#from sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dd85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#\n",
    "# The Dataset will only be available for the review\n",
    "#\n",
    "##############################\n",
    "\n",
    "\n",
    "log = pd.read_csv(\"eventsWithPhases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b702daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create log with Case ID based on currentQuestion + participant\n",
    "\n",
    "#first change data type of currentQuestion from int to str\n",
    "log = log.astype({'currentQuestion': str})\n",
    "log.dtypes\n",
    "\n",
    "#combine two columns\n",
    "log['case_id'] = pd.factorize(log.participant+log.currentQuestion)[0]\n",
    "\n",
    "#there are 614 cases, although 46 participants * 14 questions = 616 --> 2 cases are missing \n",
    "#print(len(log.case_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d396445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversion function\n",
    "def convert_ms_to_date(milliseconds):\n",
    "    date_obj = datetime.fromtimestamp(milliseconds / 1000.0)\n",
    "    date_string = date_obj.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    return date_string\n",
    "\n",
    "# Apply conversion function to 'milliseconds' column\n",
    "log['fixation_start'] = log['Fixation Start'].apply(convert_ms_to_date)\n",
    "log['fixation_end'] = log['Fixation End'].apply(convert_ms_to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeb7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'tabName_element' column\n",
    "log['activity'] = le.fit_transform(log['tabName_element'])\n",
    "\n",
    "#Number of unique activity values\n",
    "#log['activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff605e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Control-flow questions\n",
    "log_select = log[['case_id', 'fixation_start', 'activity', 'Phase', 'Type1', 'Type2', 'Type3', 'Fixation Duration']]\n",
    "log_tasks = log_select.loc[log_select['Type2'] == 'Control-flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca333c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de673391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>fixation_start</th>\n",
       "      <th>activity</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Type1</th>\n",
       "      <th>Type2</th>\n",
       "      <th>Type3</th>\n",
       "      <th>Fixation Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31794</th>\n",
       "      <td>106</td>\n",
       "      <td>1970-01-01 01:38:40.855554</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>83.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31795</th>\n",
       "      <td>106</td>\n",
       "      <td>1970-01-01 01:38:47.950454</td>\n",
       "      <td>412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>66.624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case_id              fixation_start  activity Phase  Type1  \\\n",
       "31794      106  1970-01-01 01:38:40.855554        50   NaN  Local   \n",
       "31795      106  1970-01-01 01:38:47.950454       412   NaN  Local   \n",
       "\n",
       "              Type2     Type3  Fixation Duration  \n",
       "31794  Control-flow  Ordering             83.316  \n",
       "31795  Control-flow  Ordering             66.624  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The experiment session with case_id 106 only contains 2 fixations and is therefore removed from the evaluation\n",
    "#The error porbably occured since the participant involuntarily clicked on the 'next' button to go to the next task\n",
    "log_tasks[log_tasks['case_id'] == 106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1f823b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>fixation_start</th>\n",
       "      <th>activity</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Type1</th>\n",
       "      <th>Type2</th>\n",
       "      <th>Type3</th>\n",
       "      <th>Fixation Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:15.800704</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>83.4080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:16.708983</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>124.9670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:16.883977</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>66.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:17.900487</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>83.3050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:18.375424</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>108.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173748</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:17.900596</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>191.6515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173749</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.142225</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>283.3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173750</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.442194</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>141.6530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173751</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.642171</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>191.6490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173752</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.933816</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>508.2945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98671 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        case_id              fixation_start  activity   Phase   Type1  \\\n",
       "0             0  1970-01-01 02:35:15.800704        82  search   Local   \n",
       "1             0  1970-01-01 02:35:16.708983        82  search   Local   \n",
       "2             0  1970-01-01 02:35:16.883977        82  search   Local   \n",
       "3             0  1970-01-01 02:35:17.900487        82  search   Local   \n",
       "4             0  1970-01-01 02:35:18.375424        82  search   Local   \n",
       "...         ...                         ...       ...     ...     ...   \n",
       "173748      610  1970-01-01 03:15:17.900596        88     NaN  Global   \n",
       "173749      610  1970-01-01 03:15:18.142225       179     NaN  Global   \n",
       "173750      610  1970-01-01 03:15:18.442194       179     NaN  Global   \n",
       "173751      610  1970-01-01 03:15:18.642171        50     NaN  Global   \n",
       "173752      610  1970-01-01 03:15:18.933816       179     NaN  Global   \n",
       "\n",
       "               Type2          Type3  Fixation Duration  \n",
       "0       Control-flow  Exclusiveness            83.4080  \n",
       "1       Control-flow  Exclusiveness           124.9670  \n",
       "2       Control-flow  Exclusiveness            66.6000  \n",
       "3       Control-flow  Exclusiveness            83.3050  \n",
       "4       Control-flow  Exclusiveness           108.3100  \n",
       "...              ...            ...                ...  \n",
       "173748  Control-flow       Ordering           191.6515  \n",
       "173749  Control-flow       Ordering           283.3150  \n",
       "173750  Control-flow       Ordering           141.6530  \n",
       "173751  Control-flow       Ordering           191.6490  \n",
       "173752  Control-flow       Ordering           508.2945  \n",
       "\n",
       "[98671 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_tasks = log_tasks.drop(log_tasks[log_tasks['case_id'] == 106].index)\n",
    "log_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "118b1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tasks['taskType'] = log_tasks['Type1'] + log_tasks['Type3']\n",
    "log_group = log_tasks.groupby(['case_id'])['taskType'].apply(list).reset_index()\n",
    "log_group['task'] = log_group['taskType'].apply(lambda x: str(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9b0f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[82, 82, 82, 82, 82, 177, 82, 82, 82, 82, 82, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[82, 82, 82, 181, 82, 82, 82, 82, 82, 82, 181,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[90, 82, 183, 82, 82, 82, 82, 82, 183, 77, 75,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>[86, 82, 82, 82, 82, 178, 178, 82, 82, 82, 82,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>606</td>\n",
       "      <td>[82, 59, 120, 122, 183, 120, 120, 183, 59, 183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>607</td>\n",
       "      <td>[82, 82, 82, 102, 175, 175, 175, 175, 175, 175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>608</td>\n",
       "      <td>[86, 86, 82, 108, 109, 86, 178, 86, 178, 108, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>609</td>\n",
       "      <td>[82, 195, 195, 195, 195, 195, 195, 195, 195, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>610</td>\n",
       "      <td>[82, 82, 112, 112, 179, 82, 179, 82, 179, 82, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case_id                                           activity\n",
       "0          0  [82, 82, 82, 82, 82, 177, 82, 82, 82, 82, 82, ...\n",
       "1          4  [82, 82, 82, 181, 82, 82, 82, 82, 82, 82, 181,...\n",
       "2          5  [90, 82, 183, 82, 82, 82, 82, 82, 183, 77, 75,...\n",
       "3          6  [82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8...\n",
       "4          7  [86, 82, 82, 82, 82, 178, 178, 82, 82, 82, 82,...\n",
       "..       ...                                                ...\n",
       "344      606  [82, 59, 120, 122, 183, 120, 120, 183, 59, 183...\n",
       "345      607  [82, 82, 82, 102, 175, 175, 175, 175, 175, 175...\n",
       "346      608  [86, 86, 82, 108, 109, 86, 178, 86, 178, 108, ...\n",
       "347      609  [82, 195, 195, 195, 195, 195, 195, 195, 195, 1...\n",
       "348      610  [82, 82, 112, 112, 179, 82, 179, 82, 179, 82, ...\n",
       "\n",
       "[349 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create trace log\n",
    "logVar = log_tasks.groupby(['case_id'])['activity'].apply(list).reset_index()\n",
    "#len(logVar[\"activity\"][310])\n",
    "logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2395b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861f2374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar[\"length\"] = logVar[\"activity\"].apply(lambda x: len(x))\n",
    "#logVar.sort_values(by=['length'])\n",
    "logVar['length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b347e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar['length'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4777137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5,\n",
       " 1: 0,\n",
       " 2: 3,\n",
       " 3: 4,\n",
       " 4: 7,\n",
       " 5: 6,\n",
       " 6: 2,\n",
       " 7: 1,\n",
       " 8: 5,\n",
       " 9: 0,\n",
       " 10: 3,\n",
       " 11: 4,\n",
       " 12: 7,\n",
       " 13: 6,\n",
       " 14: 2,\n",
       " 15: 1,\n",
       " 16: 5,\n",
       " 17: 0,\n",
       " 18: 3,\n",
       " 19: 4,\n",
       " 20: 7,\n",
       " 21: 6,\n",
       " 22: 2,\n",
       " 23: 1,\n",
       " 24: 5,\n",
       " 25: 0,\n",
       " 26: 3,\n",
       " 27: 4,\n",
       " 28: 7,\n",
       " 29: 6,\n",
       " 30: 2,\n",
       " 31: 1,\n",
       " 32: 5,\n",
       " 33: 0,\n",
       " 34: 3,\n",
       " 35: 4,\n",
       " 36: 7,\n",
       " 37: 6,\n",
       " 38: 2,\n",
       " 39: 1,\n",
       " 40: 5,\n",
       " 41: 0,\n",
       " 42: 3,\n",
       " 43: 4,\n",
       " 44: 7,\n",
       " 45: 6,\n",
       " 46: 2,\n",
       " 47: 1,\n",
       " 48: 5,\n",
       " 49: 0,\n",
       " 50: 3,\n",
       " 51: 4,\n",
       " 52: 7,\n",
       " 53: 6,\n",
       " 54: 2,\n",
       " 55: 1,\n",
       " 56: 5,\n",
       " 57: 0,\n",
       " 58: 3,\n",
       " 59: 4,\n",
       " 60: 7,\n",
       " 61: 2,\n",
       " 62: 1,\n",
       " 63: 5,\n",
       " 64: 0,\n",
       " 65: 3,\n",
       " 66: 4,\n",
       " 67: 7,\n",
       " 68: 6,\n",
       " 69: 2,\n",
       " 70: 1,\n",
       " 71: 5,\n",
       " 72: 0,\n",
       " 73: 3,\n",
       " 74: 4,\n",
       " 75: 7,\n",
       " 76: 6,\n",
       " 77: 2,\n",
       " 78: 1,\n",
       " 79: 5,\n",
       " 80: 0,\n",
       " 81: 3,\n",
       " 82: 4,\n",
       " 83: 7,\n",
       " 84: 6,\n",
       " 85: 2,\n",
       " 86: 1,\n",
       " 87: 5,\n",
       " 88: 0,\n",
       " 89: 3,\n",
       " 90: 4,\n",
       " 91: 7,\n",
       " 92: 6,\n",
       " 93: 2,\n",
       " 94: 1,\n",
       " 95: 5,\n",
       " 96: 0,\n",
       " 97: 3,\n",
       " 98: 4,\n",
       " 99: 7,\n",
       " 100: 6,\n",
       " 101: 2,\n",
       " 102: 1,\n",
       " 103: 5,\n",
       " 104: 0,\n",
       " 105: 3,\n",
       " 106: 4,\n",
       " 107: 7,\n",
       " 108: 6,\n",
       " 109: 2,\n",
       " 110: 1,\n",
       " 111: 5,\n",
       " 112: 0,\n",
       " 113: 3,\n",
       " 114: 4,\n",
       " 115: 7,\n",
       " 116: 6,\n",
       " 117: 2,\n",
       " 118: 1,\n",
       " 119: 5,\n",
       " 120: 0,\n",
       " 121: 3,\n",
       " 122: 4,\n",
       " 123: 7,\n",
       " 124: 6,\n",
       " 125: 2,\n",
       " 126: 1,\n",
       " 127: 5,\n",
       " 128: 0,\n",
       " 129: 3,\n",
       " 130: 4,\n",
       " 131: 7,\n",
       " 132: 6,\n",
       " 133: 2,\n",
       " 134: 1,\n",
       " 135: 5,\n",
       " 136: 0,\n",
       " 137: 3,\n",
       " 138: 4,\n",
       " 139: 7,\n",
       " 140: 6,\n",
       " 141: 2,\n",
       " 142: 1,\n",
       " 143: 5,\n",
       " 144: 0,\n",
       " 145: 3,\n",
       " 146: 4,\n",
       " 147: 7,\n",
       " 148: 6,\n",
       " 149: 2,\n",
       " 150: 1,\n",
       " 151: 5,\n",
       " 152: 0,\n",
       " 153: 3,\n",
       " 154: 4,\n",
       " 155: 7,\n",
       " 156: 6,\n",
       " 157: 2,\n",
       " 158: 1,\n",
       " 159: 5,\n",
       " 160: 0,\n",
       " 161: 3,\n",
       " 162: 4,\n",
       " 163: 7,\n",
       " 164: 6,\n",
       " 165: 2,\n",
       " 166: 1,\n",
       " 167: 5,\n",
       " 168: 0,\n",
       " 169: 3,\n",
       " 170: 4,\n",
       " 171: 7,\n",
       " 172: 6,\n",
       " 173: 2,\n",
       " 174: 1,\n",
       " 175: 5,\n",
       " 176: 0,\n",
       " 177: 3,\n",
       " 178: 4,\n",
       " 179: 7,\n",
       " 180: 6,\n",
       " 181: 2,\n",
       " 182: 1,\n",
       " 183: 5,\n",
       " 184: 0,\n",
       " 185: 3,\n",
       " 186: 4,\n",
       " 187: 7,\n",
       " 188: 6,\n",
       " 189: 2,\n",
       " 190: 1,\n",
       " 191: 5,\n",
       " 192: 0,\n",
       " 193: 3,\n",
       " 194: 4,\n",
       " 195: 7,\n",
       " 196: 6,\n",
       " 197: 2,\n",
       " 198: 1,\n",
       " 199: 5,\n",
       " 200: 0,\n",
       " 201: 3,\n",
       " 202: 4,\n",
       " 203: 7,\n",
       " 204: 6,\n",
       " 205: 2,\n",
       " 206: 1,\n",
       " 207: 5,\n",
       " 208: 0,\n",
       " 209: 3,\n",
       " 210: 4,\n",
       " 211: 7,\n",
       " 212: 6,\n",
       " 213: 2,\n",
       " 214: 1,\n",
       " 215: 5,\n",
       " 216: 0,\n",
       " 217: 3,\n",
       " 218: 4,\n",
       " 219: 7,\n",
       " 220: 6,\n",
       " 221: 2,\n",
       " 222: 1,\n",
       " 223: 5,\n",
       " 224: 0,\n",
       " 225: 3,\n",
       " 226: 4,\n",
       " 227: 7,\n",
       " 228: 6,\n",
       " 229: 2,\n",
       " 230: 1,\n",
       " 231: 5,\n",
       " 232: 0,\n",
       " 233: 3,\n",
       " 234: 4,\n",
       " 235: 7,\n",
       " 236: 6,\n",
       " 237: 2,\n",
       " 238: 1,\n",
       " 239: 5,\n",
       " 240: 0,\n",
       " 241: 3,\n",
       " 242: 4,\n",
       " 243: 7,\n",
       " 244: 6,\n",
       " 245: 2,\n",
       " 246: 1,\n",
       " 247: 5,\n",
       " 248: 0,\n",
       " 249: 3,\n",
       " 250: 4,\n",
       " 251: 7,\n",
       " 252: 6,\n",
       " 253: 2,\n",
       " 254: 1,\n",
       " 255: 5,\n",
       " 256: 0,\n",
       " 257: 3,\n",
       " 258: 4,\n",
       " 259: 7,\n",
       " 260: 6,\n",
       " 261: 2,\n",
       " 262: 1,\n",
       " 263: 5,\n",
       " 264: 0,\n",
       " 265: 3,\n",
       " 266: 4,\n",
       " 267: 7,\n",
       " 268: 6,\n",
       " 269: 2,\n",
       " 270: 1,\n",
       " 271: 5,\n",
       " 272: 0,\n",
       " 273: 3,\n",
       " 274: 4,\n",
       " 275: 7,\n",
       " 276: 6,\n",
       " 277: 2,\n",
       " 278: 1,\n",
       " 279: 5,\n",
       " 280: 0,\n",
       " 281: 3,\n",
       " 282: 4,\n",
       " 283: 7,\n",
       " 284: 6,\n",
       " 285: 2,\n",
       " 286: 1,\n",
       " 287: 5,\n",
       " 288: 0,\n",
       " 289: 3,\n",
       " 290: 4,\n",
       " 291: 7,\n",
       " 292: 6,\n",
       " 293: 2,\n",
       " 294: 1,\n",
       " 295: 5,\n",
       " 296: 0,\n",
       " 297: 3,\n",
       " 298: 4,\n",
       " 299: 7,\n",
       " 300: 6,\n",
       " 301: 2,\n",
       " 302: 1,\n",
       " 303: 5,\n",
       " 304: 0,\n",
       " 305: 3,\n",
       " 306: 4,\n",
       " 307: 7,\n",
       " 308: 6,\n",
       " 309: 2,\n",
       " 310: 1,\n",
       " 311: 5,\n",
       " 312: 0,\n",
       " 313: 3,\n",
       " 314: 4,\n",
       " 315: 7,\n",
       " 316: 6,\n",
       " 317: 2,\n",
       " 318: 1,\n",
       " 319: 5,\n",
       " 320: 0,\n",
       " 321: 3,\n",
       " 322: 4,\n",
       " 323: 7,\n",
       " 324: 6,\n",
       " 325: 2,\n",
       " 326: 1,\n",
       " 327: 5,\n",
       " 328: 0,\n",
       " 329: 3,\n",
       " 330: 4,\n",
       " 331: 7,\n",
       " 332: 6,\n",
       " 333: 2,\n",
       " 334: 1,\n",
       " 335: 5,\n",
       " 336: 0,\n",
       " 337: 3,\n",
       " 338: 4,\n",
       " 339: 7,\n",
       " 340: 6,\n",
       " 341: 2,\n",
       " 342: 1,\n",
       " 343: 0,\n",
       " 344: 3,\n",
       " 345: 4,\n",
       " 346: 7,\n",
       " 347: 6,\n",
       " 348: 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dictionary with true labels\n",
    "log_group['label'] = le.fit_transform(log_group['task'])\n",
    "labelDict1 = log_group['label'].to_dict()\n",
    "labelDict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71421cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    44\n",
       "1    43\n",
       "2    44\n",
       "3    44\n",
       "4    44\n",
       "5    43\n",
       "6    43\n",
       "7    44\n",
       "Name: case_id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_dis = log_group.groupby(['label'])['case_id'].apply(list).reset_index()\n",
    "trace_dis['case_id'].str.len()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0863e5",
   "metadata": {},
   "source": [
    "## Evaluation based on Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8407d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = np.array([[ 0, 12,  8, 10, 12],\\n       [5,  0,  9, 14, 17],\\n       [ 8,  9,  0, 12, 10],\\n       [ 8,  9,  1, 0, 10],\\n       [ 8,  9,  5, 4, 0]])\\n\\nlabelDict2 = {0:1,1:2,2:1,3:3,4:3}\\nNearestNeighbor(x, labelDict2)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NearestNeighbor(matrix, labelDict):\n",
    "    \n",
    "    clusterLabel = set(labelDict.values())\n",
    "    clusterMeasuredCount = dict.fromkeys(list(clusterLabel), 0)\n",
    "    \n",
    "    for i in range(len(matrix)):\n",
    "        #delete 0 in array matrix[i] for distance between (i,i)\n",
    "        x = np.delete(matrix[i], i) \n",
    "        #identify min distance/value in array\n",
    "        y = min(x)\n",
    "        \n",
    "        #identify position of y AND select first position/pair appearing in the array in case there are muliple pairs with identical min distance\n",
    "        nearestNeighbor = np.where(x == y)[0] #problem if multiple positions??\n",
    "        nearestNeighbor = int(nearestNeighbor[0])\n",
    "        \n",
    "        #true Label for i\n",
    "        trueLabel = labelDict[i]\n",
    "        \n",
    "        #for label comparison add +1 to dict position if position occurs after i (because of the deletion of 0 at the beginning)\n",
    "        if nearestNeighbor >= i:\n",
    "            if trueLabel == labelDict[nearestNeighbor + 1]:\n",
    "                clusterMeasuredCount[trueLabel] += 1         \n",
    "        else:\n",
    "            if trueLabel == labelDict[nearestNeighbor]:\n",
    "                clusterMeasuredCount[trueLabel] += 1\n",
    "    \n",
    "    #Count the (true) number of traces per label/attribute\n",
    "    clusterTrueCount = {}\n",
    "    for i in clusterLabel:\n",
    "        clusterTrueCount[i] = list(labelDict.values()).count(i)\n",
    "    \n",
    "    #Divide number of nearest neighbours with identical label by the respective (true number) of traces with this label\n",
    "    metric = 0\n",
    "    for i in clusterLabel:\n",
    "        metric += clusterMeasuredCount[i] / clusterTrueCount[i]\n",
    "        \n",
    "    #print(clusterMeasuredCount, clusterTrueCount, metric)\n",
    "    \n",
    "    #print(clusterMeasuredCount)\n",
    "    #print(clusterTrueCount)\n",
    "    #print(clusterLabel)\n",
    "    #print(metric)\n",
    "    return metric / len(clusterLabel)\n",
    "\n",
    "\n",
    "'''\n",
    "x = np.array([[ 0, 12,  8, 10, 12],\n",
    "       [5,  0,  9, 14, 17],\n",
    "       [ 8,  9,  0, 12, 10],\n",
    "       [ 8,  9,  1, 0, 10],\n",
    "       [ 8,  9,  5, 4, 0]])\n",
    "\n",
    "labelDict2 = {0:1,1:2,2:1,3:3,4:3}\n",
    "NearestNeighbor(x, labelDict2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aae66",
   "metadata": {},
   "source": [
    "## Evaluation based on Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24a0d082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx3 = np.array([[ 0, 12,  8, 10, 12],\\n       [5,  0,  9, 14, 17],\\n       [ 8,  9,  0, 12, 10],\\n       [ 8,  9,  1, 0, 10],\\n       [ 4,  8,  4, 4, 0]]) # --> Issue: How to select NN when identical distance values !!!\\n\\nlabelDict3 = {0:1,1:2,2:1,3:3,4:3}\\nPrecisionAtK(x3, labelDict3, 2)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_key_by_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    #return None  # Return None if the value is not found in the dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PrecisionAtK(matrix, labelDict, k):\n",
    "    \n",
    "    clusterLabel = set(labelDict.values())\n",
    "    clusterMeasuredCount = dict.fromkeys(list(clusterLabel), 0)\n",
    "    \n",
    "    \n",
    "    for i in range(len(matrix)):\n",
    "        \n",
    "        #true Label for i\n",
    "        trueLabel = labelDict[i]\n",
    "\n",
    "        \n",
    "        #delete 0 in array matrix[i] for distance between (i,i)   \n",
    "        x = np.delete(matrix[i], i)\n",
    "        \n",
    "        #get all k minimum values\n",
    "        nnValues = []\n",
    "        for l in range(k):\n",
    "            y = min(x)\n",
    "            nnValues.append(y)\n",
    "            x = np.delete(x, np.where(x == y)[0][0])\n",
    "        \n",
    "        \n",
    "        #get location of minimum values\n",
    "        z = np.delete(matrix[i], i)\n",
    "        \n",
    "        #create dict from array with {position:value}\n",
    "        my_dict = {}\n",
    "        for m in range(len(z)):\n",
    "            my_dict[m] = z[m]\n",
    "        \n",
    "        key_list = []\n",
    "        for n in nnValues:\n",
    "            key = get_key_by_value(my_dict, n)\n",
    "            key_list.append(key)\n",
    "            del my_dict[key]\n",
    "        \n",
    "        for o in range(k):\n",
    "            position = list(key_list)[o]\n",
    "\n",
    "            if position >= i:\n",
    "                if trueLabel == labelDict[position + 1]:\n",
    "                    clusterMeasuredCount[trueLabel] += 1\n",
    "\n",
    "            else:\n",
    "                if trueLabel == labelDict[position]:\n",
    "                    clusterMeasuredCount[trueLabel] += 1\n",
    "\n",
    "            #transform array to dict\n",
    "            #get key value from dict\n",
    "            #compare label\n",
    "            #remove key+value from dict\n",
    "\n",
    "            #Need exception in case y == 0 ??\n",
    "            \n",
    "    \n",
    "    #Count the (true) number of traces per label/attribute\n",
    "    clusterTrueCount = {}\n",
    "    for i in clusterLabel:\n",
    "        clusterTrueCount[i] = list(labelDict.values()).count(i)\n",
    "    \n",
    "    #Divide number of nearest neighbours with identical label by the respective (true number) of traces with this label\n",
    "    metric = 0\n",
    "    for i in clusterLabel:\n",
    "        metric += clusterMeasuredCount[i] / k / clusterTrueCount[i]\n",
    "        \n",
    "    #print(clusterMeasuredCount, clusterTrueCount, metric)\n",
    "    \n",
    "    #print(clusterLabel)\n",
    "    #print(metric)\n",
    "    return metric / len(clusterLabel)\n",
    "\n",
    "\n",
    "'''\n",
    "x3 = np.array([[ 0, 12,  8, 10, 12],\n",
    "       [5,  0,  9, 14, 17],\n",
    "       [ 8,  9,  0, 12, 10],\n",
    "       [ 8,  9,  1, 0, 10],\n",
    "       [ 4,  8,  4, 4, 0]]) # --> Issue: How to select NN when identical distance values !!!\n",
    "\n",
    "labelDict3 = {0:1,1:2,2:1,3:3,4:3}\n",
    "PrecisionAtK(x3, labelDict3, 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18183bf",
   "metadata": {},
   "source": [
    "## Triplet\n",
    "\n",
    "see: https://towardsdatascience.com/triplet-loss-advanced-intro-49a07b7d8905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "836bd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_mask(labels):\n",
    "    \n",
    "    # step 1 - get a mask for distinct indices\n",
    "    ###print('labels', labels)\n",
    "    # shape: (batch_size, batch_size)\n",
    "    indices_equal = torch.eye(labels.size()[0], dtype=torch.bool, device=labels.device)\n",
    "    ###print('equal', indices_equal)\n",
    "    indices_not_equal = torch.logical_not(indices_equal)\n",
    "    ###print('not_equal', indices_not_equal)\n",
    "\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    i_not_equal_j = indices_not_equal.unsqueeze(2)\n",
    "    ###print('i_not_j - unsqueeze2', i_not_equal_j)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    i_not_equal_k = indices_not_equal.unsqueeze(1)\n",
    "    ###print('i_not_k - unsqueeze1', i_not_equal_k)\n",
    "    # shape: (1, batch_size, batch_size)\n",
    "    j_not_equal_k = indices_not_equal.unsqueeze(0)\n",
    "    ###print('j_not_k - unsqueeze0', i_not_equal_k)\n",
    "    # Shape: (batch_size, batch_size, batch_size)\n",
    "    distinct_indices = torch.logical_and(torch.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "    ###print('distinct!!!!', distinct_indices)\n",
    "\n",
    "    # step 2 - get a mask for valid anchor-positive-negative triplets\n",
    "    # shape: (batch_size, batch_size)\n",
    "    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "    ###print('labels_equal', labels_equal)\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    i_equal_j = labels_equal.unsqueeze(2)\n",
    "    ###print('i_equal_j', i_equal_j)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    i_equal_k = labels_equal.unsqueeze(1)\n",
    "    ###print('i_equal_k', i_equal_k)\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    valid_indices = torch.logical_and(i_equal_j, torch.logical_not(i_equal_k))\n",
    "    ###print('valid_indices!!!', valid_indices)\n",
    "    \n",
    "\n",
    "    # step 3 - combine two masks\n",
    "    mask = torch.logical_and(distinct_indices, valid_indices)\n",
    "    ###print('mask!!', mask)\n",
    "    return mask\n",
    "\n",
    "    \"\"\"compute a mask for valid triplets\n",
    "    Args:\n",
    "        labels: Batch of integer labels. shape: (batch_size,)\n",
    "    Returns:\n",
    "        Mask tensor to indicate which triplets are actually valid. Shape: (batch_size, batch_size, batch_size)\n",
    "        A triplet is valid if:\n",
    "        `labels[i] == labels[j] and labels[i] != labels[k]`\n",
    "        and `i`, `j`, `k` are different.\n",
    "    \"\"\"\n",
    "    \n",
    "class custom_activation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_activation, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x[x>0] = 1\n",
    "        x[x<=0] = 0\n",
    "        return x\n",
    "\n",
    "\n",
    "class BatchAllTtripletLoss(nn.Module):\n",
    "  \"\"\"Uses all valid triplets to compute Triplet loss\n",
    "\n",
    "  Args:\n",
    "    margin: Margin value in the Triplet Loss equation\n",
    "  \"\"\"\n",
    "  def __init__(self, margin=0): #default margin = 0\n",
    "    super().__init__()\n",
    "    self.margin = margin\n",
    "    self.relu = nn.ReLU() #new\n",
    "    self.custom = custom_activation()\n",
    "    \n",
    "  def forward(self, distance_matrix, labels):\n",
    "    \"\"\"computes loss value.\n",
    "\n",
    "    Args:\n",
    "      embeddings: Batch of embeddings, e.g., output of the encoder. shape: (batch_size, embedding_dim)\n",
    "      labels: Batch of integer labels associated with embeddings. shape: (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "      Scalar loss value.\n",
    "    \"\"\"\n",
    "    # step 1 - convert to tensor format\n",
    "    distance_matrix = torch.tensor(distance_matrix)\n",
    "    labels = torch.tensor(list(labels.values()))\n",
    "\n",
    "\n",
    "    # step 2 - compute loss values for all triplets by applying broadcasting to distance matrix\n",
    "\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    anchor_positive_dists = distance_matrix.unsqueeze(2)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    anchor_negative_dists = distance_matrix.unsqueeze(1)\n",
    "    # get loss values for all possible n^3 triplets\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    triplet_loss = anchor_negative_dists - anchor_positive_dists + self.margin\n",
    "    ###print('tl0',triplet_loss)\n",
    "\n",
    "    # step 3 - filter out invalid or easy triplets by setting their loss values to 0\n",
    "\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    mask = get_triplet_mask(labels)\n",
    "    ###print('mask', mask)\n",
    "    triplet_loss *= mask\n",
    "    ###print(triplet_loss)\n",
    "    ###print('tl1:', triplet_loss)\n",
    "    # easy triplets have negative loss values\n",
    "    \n",
    "    triplet_loss = self.custom(triplet_loss)\n",
    "    ###print(triplet_loss)\n",
    "    #triplet_loss = F.relu(triplet_loss)\n",
    "\n",
    "    # step 4 - compute scalar loss value by averaging positive losses\n",
    "    \n",
    "    triLossNonZero = (triplet_loss != 0).nonzero(as_tuple=True)\n",
    "    labelTorchUnique = torch.unique(labels, return_counts=True)\n",
    "    \n",
    "    nonZero = len(triLossNonZero[0])\n",
    "    triLossSum = []\n",
    "    for i in range(nonZero):\n",
    "        #Identify L_a --> In Class\n",
    "        t1 = triLossNonZero[0][i]\n",
    "        labelIn = labels[t1]\n",
    "        positionIn = int((labelTorchUnique[0] == labelIn).nonzero(as_tuple=False))\n",
    "        countIn = labelTorchUnique[1][positionIn]\n",
    "\n",
    "        #Identify L_b --> Out Class\n",
    "        t3 = triLossNonZero[2][i]\n",
    "        labelOut = labels[t3]\n",
    "        positionOut = int((labelTorchUnique[0] == labelOut).nonzero(as_tuple=False))\n",
    "        countOut = labelTorchUnique[1][positionOut]\n",
    "\n",
    "        #Calculate loss\n",
    "        value = (1/countIn)*(1/countIn)*(1/countOut)  \n",
    "        ###print(countIn)\n",
    "        ###print(countOut)\n",
    "        triLossSum.append(value)\n",
    "    \n",
    "    #finally divide by |A|^2-|A|\n",
    "    A = len(labelTorchUnique[0])  \n",
    "    lossValue = sum(triLossSum) / (A*A-A)\n",
    "        \n",
    "    #OLD\n",
    "    #E_triplet = (1 / (A^2 - A)) *\n",
    "    #num_positive_losses = (triplet_loss > eps).float().sum()\n",
    "    #print(num_positive_losses)\n",
    "    #print(triplet_loss.sum())\n",
    "    #triplet_loss = triplet_loss.sum() / (num_positive_losses + eps)\n",
    "    \n",
    "\n",
    "    return lossValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12a92e",
   "metadata": {},
   "source": [
    "## Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26715870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def Silhouette(distMatrix, labelDict):\n",
    "    labelDictList = list(labelDict.values())\n",
    "    return metrics.silhouette_score(distMatrix, labelDictList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a41b8",
   "metadata": {},
   "source": [
    "## Ground truth comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28bc49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary with true labels\n",
    "log_group['label'] = le.fit_transform(log_group['task'])\n",
    "labelDict1 = log_group['label'].to_dict()\n",
    "#labelDict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b448163",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"c:n_chr\"] = logVar[\"activity\"].apply(lambda x: [chr(i) for i in x])\n",
    "logVar[\"strings\"] = logVar[\"c:n_chr\"].apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c563b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_calc(features, distance):\n",
    "    n = len(features)\n",
    "    dist_matrix = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix[i,j] = distance(features[i], features[j])\n",
    "            dist_matrix[j,i] = dist_matrix[i,j]\n",
    "    \n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf525c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(DistMatrix):\n",
    "    print('NN:   ' + str(NearestNeighbor(DistMatrix, labelDict1)))\n",
    "    print('P@10: ' + str(PrecisionAtK(DistMatrix, labelDict1, 10)))\n",
    "\n",
    "    triplet = BatchAllTtripletLoss()\n",
    "    print('Tri:  ' + str(triplet.forward(DistMatrix,labelDict1)))\n",
    "    print('Sil:  ' + str(Silhouette(DistMatrix, labelDict1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f908a1",
   "metadata": {},
   "source": [
    "### Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "babbef01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 267, 316, ..., 263, 274, 332],\n",
       "       [267,   0, 311, ..., 188, 180, 311],\n",
       "       [316, 311,   0, ..., 333, 307, 319],\n",
       "       ...,\n",
       "       [263, 188, 333, ...,   0, 194, 345],\n",
       "       [274, 180, 307, ..., 194,   0, 351],\n",
       "       [332, 311, 319, ..., 345, 351,   0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "List = list(logVar[\"strings\"])\n",
    "\n",
    "dist_matrix = np.zeros((len(List),len(List)),dtype=int)\n",
    "\n",
    "for i in range(0,len(List)):\n",
    "    for j in range(0,len(List)):\n",
    "        dist_matrix[i,j] = distance(List[i],List[j])\n",
    "\n",
    "lev_dis = dist_matrix\n",
    "lev_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "167bdc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.8972647991543341\n",
      "P@10: 0.7623678646934461\n",
      "Tri:  tensor(0.6691)\n",
      "Sil:  -0.22453133331372324\n"
     ]
    }
   ],
   "source": [
    "results(lev_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1e509",
   "metadata": {},
   "source": [
    "### Normalized Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15cb3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "List = logVar[\"strings\"]\n",
    "\n",
    "n = len(List)\n",
    "dist_matrix = np.zeros((n,n))    # initialize distance matrix to a square of zeros\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i, n):\n",
    "        dist_matrix[i,j] = distance(List[i], List[j]) / max(len(List[i]),len(List[j]))\n",
    "        dist_matrix[j,i] = dist_matrix[i,j]       # for the symmetric part, no computation\n",
    "\n",
    "lev_dis_norm = dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e1e0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9371696617336153\n",
      "P@10: 0.8652946617336154\n",
      "Tri:  tensor(0.8727)\n",
      "Sil:  0.10736466133082465\n"
     ]
    }
   ],
   "source": [
    "results(lev_dis_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c1c26",
   "metadata": {},
   "source": [
    "### Cosine based on 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af2a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 1-gram\n",
    "\n",
    "def createVector(charList):\n",
    "    #dtype = [('structure', 'S10'), ('relfrequ', float)]\n",
    "    arrayList = np.array(charList)\n",
    "    unique, counts = np.unique(arrayList, return_counts=True)\n",
    "    #calculate relative frequency\n",
    "    relFrequList = np.array((unique, counts)).T\n",
    "    uniqueList = list(unique)\n",
    "    return relFrequList[relFrequList[:, 0].argsort()]\n",
    "    #check completeness\n",
    "    #if 'tree' not in uniqueList:\n",
    "        #relFrequList = np.append(relFrequList, np.array([['tree', 0]]), axis=0)\n",
    "        #print(relFrequList)\n",
    "\n",
    "        \n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"1-gram\"] = logVar[\"c:n_chr\"].apply(lambda x: createVector(tuple(x)))\n",
    "#logVar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13f472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignArrays(array1, array2):\n",
    "    commonSet = set(array1[:,0]).union(array2[:,0])\n",
    "    \n",
    "    for i in commonSet:\n",
    "        if i not in array1[:,0]:\n",
    "            array1 = np.append(array1, np.array([[i, '0']]), axis=0)\n",
    "        if i not in array2[:,0]:\n",
    "            array2 = np.append(array2, np.array([[i, '0']]), axis=0)\n",
    "    return array1[array1[:, 0].argsort()], array2[array2[:, 0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84280423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cosineDist(frequVector1, frequVector2):\n",
    "    Vector1, Vector2 = alignArrays(frequVector1, frequVector2)\n",
    "    a = Vector1[:,1].astype(int)\n",
    "    b = Vector2[:,1].astype(int)\n",
    "    dist_matrix = distance.cosine(a, b)\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b437c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine distance based on 1-gram\n",
    "\n",
    "cos1_dis = matrix_calc(logVar[\"1-gram\"],cosineDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d60975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9772727272727273\n",
      "P@10: 0.9241477272727272\n",
      "Tri:  tensor(0.8901)\n",
      "Sil:  0.22334463230599214\n"
     ]
    }
   ],
   "source": [
    "results(cos1_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535cee1",
   "metadata": {},
   "source": [
    "### Cosine based on 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef182177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data format from string to list of unique characters\n",
    "#logVar[\"charList\"] = logVar[\"trace_variant\"].apply(lambda x: list(x))\n",
    "#logVar\n",
    "\n",
    "def df_list(list_of_char):\n",
    "    extList = list_of_char.copy()\n",
    "    extList.insert(0, '*') \n",
    "    extList.append('$')\n",
    "    list_new = []\n",
    "    for i in range(len(extList)):\n",
    "        new = ''.join(extList[i:i+2])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    return list_new\n",
    "\n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"dfList\"] = logVar[\"c:n_chr\"].apply(lambda x: df_list(x))\n",
    "#logVar\n",
    "\n",
    "logVar[\"2-gram\"] = logVar[\"dfList\"].apply(lambda x: createVector(x))\n",
    "#logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "888a3701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9515063424947147\n",
      "P@10: 0.8421643763213531\n",
      "Tri:  tensor(0.8221)\n",
      "Sil:  0.08707110224755636\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on 2-gram\n",
    "\n",
    "cos2_dis = matrix_calc(logVar[\"2-gram\"],cosineDist)\n",
    "results(cos2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "956c7dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create 3-gram\n",
    "\n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"charList\"] = logVar[\"c:n_chr\"].apply(lambda x: list(x))\n",
    "\n",
    "def df_list2(list_of_char):\n",
    "    extList = list_of_char.copy()\n",
    "    extList.insert(0, '*') \n",
    "    extList.append('$')\n",
    "    list_new = []\n",
    "    for i in range(len(extList) - 1):\n",
    "        new = ''.join(extList[i:i+3])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    return list_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74b9479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"dfList2\"] = logVar[\"charList\"].apply(lambda x: df_list2(x))\n",
    "logVar[\"3-gram\"] = logVar[\"dfList2\"].apply(lambda x: createVector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b0086b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.8682610993657506\n",
      "P@10: 0.7532835623678646\n",
      "Tri:  tensor(0.7620)\n",
      "Sil:  -0.0012724927871462736\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on 3-gram\n",
    "\n",
    "cos3_dis = matrix_calc(logVar[\"3-gram\"],cosineDist)\n",
    "results(cos3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb20c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9715248414376322\n",
      "P@10: 0.8948929704016915\n",
      "Tri:  tensor(0.8620)\n",
      "Sil:  0.1573427550043208\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "aggregate1 = cos1_dis + cos2_dis\n",
    "results(aggregate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e3c3fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9543472515856237\n",
      "P@10: 0.8604981501057083\n",
      "Tri:  tensor(0.8380)\n",
      "Sil:  0.10582398018892779\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "aggregate2 = cos1_dis + cos2_dis + cos3_dis\n",
    "results(aggregate2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b76886",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a28527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "# see https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "\n",
    "def euclidDist(frequVector1, frequVector2):\n",
    "    Vector1, Vector2 = alignArrays(frequVector1, frequVector2)\n",
    "    a = Vector1[:,1].astype(float)\n",
    "    b = Vector2[:,1].astype(float)\n",
    "    euclidean_dist = np.linalg.norm(a-b)\n",
    "    return euclidean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2331cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9542811839323467\n",
      "P@10: 0.8622225158562368\n",
      "Tri:  tensor(0.7647)\n",
      "Sil:  -0.05617607680390533\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 1-gram\n",
    "\n",
    "euc1_dis = matrix_calc(logVar[\"1-gram\"],euclidDist)\n",
    "results(euc1_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7087bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.885438689217759\n",
      "P@10: 0.7075184989429175\n",
      "Tri:  tensor(0.6804)\n",
      "Sil:  -0.12094105926059776\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 2-gram\n",
    "\n",
    "euc2_dis = matrix_calc(logVar[\"2-gram\"],euclidDist)\n",
    "results(euc2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4efd5813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.6762024312896405\n",
      "P@10: 0.5240420190274842\n",
      "Tri:  tensor(0.6136)\n",
      "Sil:  -0.14871501783104946\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 3-gram\n",
    "\n",
    "euc3_dis = matrix_calc(logVar[\"3-gram\"],euclidDist)\n",
    "results(euc3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "361e5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9427193446088795\n",
      "P@10: 0.8232756342494715\n",
      "Tri:  tensor(0.7365)\n",
      "Sil:  -0.08381334058999264\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "agg_euc = euc1_dis + euc2_dis\n",
    "results(agg_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaef19c",
   "metadata": {},
   "source": [
    "### Jaccard based on 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "feb39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance based on activity type\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1, s2 = set(list1), set(list2)\n",
    "    return 1 - len(s1 & s2) / len(s1 | s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "916e7de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9655787526427062\n",
      "P@10: 0.9051268498942917\n",
      "Tri:  tensor(0.8467)\n",
      "Sil:  0.15975306042991522\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 1-gram\n",
    "Jacc1_dis = matrix_calc(logVar[\"charList\"],jaccard_similarity)\n",
    "results(Jacc1_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12164c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9827563424947146\n",
      "P@10: 0.9583047040169133\n",
      "Tri:  tensor(0.9151)\n",
      "Sil:  0.11957225318299027\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 2-gram\n",
    "Jacc2_dis = matrix_calc(logVar[\"dfList\"],jaccard_similarity)\n",
    "results(Jacc2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58612249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9857293868921776\n",
      "P@10: 0.9660280126849894\n",
      "Tri:  tensor(0.9193)\n",
      "Sil:  0.05107219772286129\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 3-gram\n",
    "Jacc3_dis = matrix_calc(logVar[\"dfList2\"],jaccard_similarity)\n",
    "results(Jacc3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ab92a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9886363636363636\n",
      "P@10: 0.9360002642706132\n",
      "Tri:  tensor(0.8779)\n",
      "Sil:  0.15678760609316028\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "agg_jacc1 = Jacc1_dis + Jacc2_dis\n",
    "results(agg_jacc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c759b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9885702959830867\n",
      "P@10: 0.9520084566596195\n",
      "Tri:  tensor(0.8912)\n",
      "Sil:  0.1305832957211289\n"
     ]
    }
   ],
   "source": [
    "agg_jacc2 = Jacc1_dis + Jacc2_dis + Jacc3_dis\n",
    "results(agg_jacc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607bdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e67795",
   "metadata": {},
   "source": [
    "## Graph based measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now consider edge types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26f94a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intEncoder(character_List):\n",
    "    return [np.where(np.array(list(dict.fromkeys(character_List)))==e)[0][0]for e in character_List]\n",
    "\n",
    "logVar[\"intList\"] = logVar[\"activity\"].apply(lambda x: intEncoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "26a1b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. transfer intList to int_tupleList\n",
    "\n",
    "#Create tuple lists\n",
    "def tuple_list(list_of_encodedActivities):\n",
    "    #list.insert(0, '*')\n",
    "    #list.append('*')\n",
    "    list_new = []\n",
    "    last_element = list_of_encodedActivities[-1]\n",
    "    for i in range(len(list_of_encodedActivities)):\n",
    "        new = tuple(list_of_encodedActivities[i:i+2])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    if list_of_encodedActivities.count(last_element) == 1: #check wether last activity in trace has some adjancency relation\n",
    "        list_new.append((last_element,)) ### NOT Correct\n",
    "    return list_new\n",
    "\n",
    "#q = [0,0,0,0,1,1,2,3,4,5,3,2,4,0,5,6]\n",
    "#tuple_list(q)\n",
    "\n",
    "logVar[\"int_tupleList\"] = logVar[\"intList\"].apply(lambda x: tuple_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4db1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. generate Adjacency List\n",
    "\n",
    "def adj_list(list_of_tuples):\n",
    "    adj_list_new = {}\n",
    "    try:\n",
    "        for node1, node2 in list_of_tuples:\n",
    "            #print(node1, node2)\n",
    "            if node1 not in adj_list_new:\n",
    "                newlist = []\n",
    "                newlist.append(node2)\n",
    "                adj_list_new[node1] = newlist\n",
    "                #print(adj_list3)\n",
    "        \n",
    "            else:\n",
    "                if node2 not in adj_list_new[node1]:\n",
    "                    #mylist.extend(adj_list3[node1])\n",
    "                    adj_list_new[node1].append(node2)\n",
    "                    #print(adj_list3)\n",
    "                    #adj_list3[node1] = mylist\n",
    "    \n",
    "    #in case activity has no adjacent activity - only possible for last activity --> tuple: (lastAct,)\n",
    "    except ValueError as ve:\n",
    "        lastValue = list_of_tuples[-1][0] \n",
    "        adj_list_new[lastValue] = list()\n",
    "    return list(adj_list_new.values())\n",
    "\n",
    "#q = [0,0,0,0,1,1,2,3,4,5,3,2,4,0,5,6]\n",
    "#l = tuple_list(q)\n",
    "#adj_list(l)\n",
    "\n",
    "logVar[\"int_adjList\"] = logVar[\"int_tupleList\"].apply(lambda x: adj_list(x))\n",
    "#logVar[\"int_adjList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ad9bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now consider length\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs_4(graph, start, end):\n",
    "    \n",
    "    graph = {v: k for v, k in enumerate(graph)}\n",
    "    #print(start, end)\n",
    "    queue = deque([(start, 0)])\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        #print(queue)\n",
    "        node, distance = queue.popleft()\n",
    "        #if not node:\n",
    "            #print(start, end, queue)\n",
    "            #print(\"GRAPH LIST\", graph)\n",
    "        if node in seen:\n",
    "            continue\n",
    "        seen.add(node)\n",
    "        if node == end:\n",
    "            return distance \n",
    "        for adjacent in graph.get(node, []):\n",
    "            queue.append((adjacent, distance + 1))\n",
    "        \n",
    "#x = {0: [0, 1], 1: [2, 1, 0, int], 2:[2], [3: [1, 5, 3, 7], 4: [3], 5: [6, 5], 6: [1, 7], 7: [8, 9, 7], 8: [5, 8, 10], 9: [3]}\n",
    "#y = [[0, 1, 5], [1, 2], [3, 4], [4, 2], [5, 0], [3, 6], []]\n",
    "#bfs_4(y, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8cbbc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import copy\n",
    "\n",
    "def reverse_graph(graph):\n",
    "    reversed_graph = defaultdict(list)\n",
    "    for node in graph:\n",
    "        for neighbor in graph[node]:\n",
    "            reversed_graph[neighbor].append(node)\n",
    "    return reversed_graph\n",
    "\n",
    "\n",
    "def bfs_5(graph, start, end):\n",
    "    queue = deque([(start, 0)])\n",
    "    seen = set()\n",
    "    visited = {}\n",
    "    while queue:\n",
    "        node, distance = queue.popleft()\n",
    "        if node in seen:\n",
    "            continue\n",
    "        seen.add(node)\n",
    "        if node == end: # maybe quicker if adjacent directly checked\n",
    "            return visited\n",
    "        for adjacent in graph.get(node, []):\n",
    "            queue.append((adjacent, distance + 1))\n",
    "            if adjacent not in visited:\n",
    "                visited.update({adjacent:distance})\n",
    "\n",
    "            \n",
    "def common_ancestors(graph, node1, node2): \n",
    "    #remove cross type edge between node1 and node2\n",
    "    graph = copy.deepcopy(graph)\n",
    "    graph[node1].remove(node2)\n",
    "    graph = {v: k for v, k in enumerate(graph)}\n",
    "    graphReverse = reverse_graph(graph)\n",
    "    setNode1 = bfs_5(graphReverse, node1, 0)\n",
    "    setNode2 = bfs_5(graphReverse, node2, 0)\n",
    "    if next((a for a in list(setNode1) if a in list(setNode2)), None) == None:\n",
    "        firstCommonAnces = next((a for a in list(setNode2) if a in list(setNode1)), None)\n",
    "    else:\n",
    "        firstCommonAnces = next((a for a in list(setNode1) if a in list(setNode2)), 0)\n",
    "    \n",
    "    #uses a hash map to identify the first common ancestor in both lists\n",
    "    #looks for the first common ancestor in setNode1, which can also be found in setNode2 \n",
    "    #--> this might not be the closest distance between setNode1 and setNode2\n",
    "    #--> e.g., for x= [0,1,3,7,5,6] and y= [4,5,7,8,3] 7 might be closest ancestor, although algo detects 3 !\n",
    "    #distance = setNode1[firstCommonAnces] + setNode2[firstCommonAnces]\n",
    "    \n",
    "    \n",
    "    if firstCommonAnces != None:  \n",
    "        ancesDistNode1 =  setNode1[firstCommonAnces] + 1 #the edge from node1 to first parent is counted as 0 by algorithm, therefore +1\n",
    "        ancesDistNode2 =  setNode2[firstCommonAnces] + 1\n",
    "        numberSkips = abs(ancesDistNode1 - ancesDistNode2)\n",
    "        numberCross = min(ancesDistNode1, ancesDistNode2)\n",
    "    else:\n",
    "        numberSkips, numberCross = (0,1)\n",
    "    return numberSkips, numberCross\n",
    "    #if all(x in crossType for x in i):\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#graphList = [[1], [2, 4, 1], [3, 2, 1], [], [5, 4], [5, 4, 6], [7], []]\n",
    "#c = [[1, 4], [2], [3], [0, 5], [3, 5], []]\n",
    "#c2 = {v: k for v, k in enumerate(c)}\n",
    "#common_ancestors(c, 4, 5)\n",
    "#reverse_graph(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20f4dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List for decoding traces\n",
    "from collections import OrderedDict\n",
    "logVar[\"indexList\"] = logVar[\"activity\"].apply(lambda x: list(OrderedDict.fromkeys(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c6ba6",
   "metadata": {},
   "source": [
    "### Cosine Edge Type + length (no df relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "84f897bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph1:\n",
    "    # instance variables\n",
    "    def __init__(self, graph_list2, indexList):\n",
    "        # v is the number of nodes/vertices\n",
    "        self.time = 0\n",
    "        self.traversal_array = []\n",
    "        self.structural_array = [['sequ', 1]]\n",
    "        #self.structural_array = []\n",
    "        self.graph_list = graph_list2\n",
    "        self.v = len(graph_list2)\n",
    "        self.indexList = indexList\n",
    "\n",
    "    # function for dfs\n",
    "    def dfs(self):\n",
    "        self.start_time = [-1]*self.v\n",
    "        self.end_time = [-1]*self.v\n",
    " \n",
    "        for node in range(self.v):\n",
    "            if self.start_time[node] == -1:\n",
    "                self.traverse_dfs(node)\n",
    "                \n",
    "        return np.array(self.structural_array)\n",
    "        #print()\n",
    "        #print(\"DFS Traversal: \", self.traversal_array)\n",
    "        #print()\n",
    " \n",
    "    def traverse_dfs(self, node):\n",
    "        self.traversal_array.append(node)\n",
    "        # get the starting time\n",
    "        self.start_time[node] = self.time\n",
    "        self.time += 1\n",
    "        # traverse through the neighbours\n",
    "        for neighbour in self.graph_list[node]:\n",
    "\n",
    "            # when the neighbor was not yet visited\n",
    "            if self.start_time[neighbour] == -1:                \n",
    "                self.structural_array[0][1] += 0\n",
    "                self.traverse_dfs(neighbour)\n",
    "                \n",
    "            # otherwise when the neighbour's visit is still ongoing:\n",
    "            elif self.end_time[neighbour] == -1:\n",
    "                if node == neighbour:\n",
    "                    self.structural_array.append(['1back ',1])\n",
    "                    #self.structural_array.append(['back ',1])\n",
    "                    #self.structural_array.append([str(1)+'b'])\n",
    "                \n",
    "                elif node in self.graph_list[neighbour]:\n",
    "                    self.structural_array.append(['2back ',2])\n",
    "                    #self.structural_array.append(['back ',2])\n",
    "                    #self.structural_array.append(str(2)+'b')\n",
    "                    \n",
    "                else:\n",
    "                    x = bfs_4(self.graph_list, neighbour, node)\n",
    "                    self.structural_array.append([str(x+1)+'back ',x+1])\n",
    "                    #self.structural_array.append(['back ',x+1])\n",
    "                    #self.structural_array.append(str(x+1)+'b')\n",
    "                \n",
    "            # otherwise when the neighbour's visit started before the current node's visit:\n",
    "            elif self.start_time[node] < self.start_time[neighbour]:\n",
    "                graph_list_copy = copy.deepcopy(self.graph_list)\n",
    "                graph_list_copy[node].remove(neighbour)\n",
    "                y = bfs_4(graph_list_copy, node, neighbour)\n",
    "                self.structural_array.append([str(y-1)+'forward ',y-1])\n",
    "\n",
    "            else:\n",
    "                numberSkips, numberCross = common_ancestors(self.graph_list, node, neighbour)\n",
    "                self.structural_array.append([str(numberCross)+'cross ',numberCross])\n",
    "  \n",
    "    \n",
    "        # Indentation corrected:\n",
    "        self.end_time[node] = self.time\n",
    "        self.time += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b7903ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def transform_list_of_pairs(pairs):\n",
    "    return [pair[0] for pair in pairs]\n",
    "\n",
    "\n",
    "\n",
    "def count_entries(input_list):\n",
    "    # Count the occurrences of each unique entry in the list\n",
    "    counter = Counter(input_list)\n",
    "    \n",
    "    # Create a NumPy array from the counter dictionary\n",
    "    result = np.array([[key, count] for key, count in counter.items()], dtype=object)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "#input_list = ['sequ', '2back', '2back']\n",
    "#result = count_entries(input_list)\n",
    "#print(result)\n",
    "\n",
    "logVar[\"int_strucLengthList2\"] = logVar.apply(lambda x: Graph1(x.int_adjList, x.indexList).dfs(), axis =1)\n",
    "logVar[\"relFrequVec1\"] = logVar[\"int_strucLengthList2\"].apply(lambda x: transform_list_of_pairs(x))\n",
    "logVar[\"relFrequVec1\"] = logVar[\"relFrequVec1\"].apply(lambda x: count_entries(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0fdd3a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.17197410147991543\n",
      "P@10: 0.15701638477801266\n",
      "Tri:  tensor(0.5332)\n",
      "Sil:  -0.13404679651077211\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on edge types\n",
    "cos_graph_dis = matrix_calc(logVar[\"relFrequVec1\"],cosineDist)\n",
    "results(cos_graph_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "799bfe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9571220930232558\n",
      "P@10: 0.898090644820296\n",
      "Tri:  tensor(0.8625)\n",
      "Sil:  0.15896867249184918\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on edge types\n",
    "agg_cos = cos1_dis + cos2_dis + cos_graph_dis\n",
    "results(agg_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534419f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3075f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c470016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb5e8732",
   "metadata": {},
   "source": [
    "### Jaccard Edge Type and length + df relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a2b71f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph2:\n",
    "    # instance variables\n",
    "    def __init__(self, graph_list2, indexList):\n",
    "        # v is the number of nodes/vertices\n",
    "        self.time = 0\n",
    "        self.traversal_array = []\n",
    "        self.structural_array = []\n",
    "        #self.structural_array = []\n",
    "        self.graph_list = graph_list2\n",
    "        self.v = len(graph_list2)\n",
    "        self.indexList = indexList\n",
    "\n",
    "    # function for dfs\n",
    "    def dfs(self):\n",
    "        self.start_time = [-1]*self.v\n",
    "        self.end_time = [-1]*self.v\n",
    " \n",
    "        for node in range(self.v):\n",
    "            if self.start_time[node] == -1:\n",
    "                self.traverse_dfs(node)\n",
    "                \n",
    "        return self.structural_array\n",
    "        #print()\n",
    "        #print(\"DFS Traversal: \", self.traversal_array)\n",
    "        #print()\n",
    " \n",
    "    def traverse_dfs(self, node):\n",
    "        self.traversal_array.append(node)\n",
    "        # get the starting time\n",
    "        self.start_time[node] = self.time\n",
    "        self.time += 1\n",
    "        # traverse through the neighbours\n",
    "        for neighbour in self.graph_list[node]:\n",
    "\n",
    "            # when the neighbor was not yet visited\n",
    "            if self.start_time[neighbour] == -1:                \n",
    "                #self.structural_array[0][1] += 0\n",
    "                #self.structural_array.append('tree ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ')\n",
    "                self.structural_array.append('tree')\n",
    "                self.traverse_dfs(neighbour)\n",
    "                \n",
    "            # otherwise when the neighbour's visit is still ongoing:\n",
    "            elif self.end_time[neighbour] == -1:\n",
    "                if node == neighbour:\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(1))\n",
    "                    #self.structural_array.append([str(1)+'b'])\n",
    "                \n",
    "                elif node in self.graph_list[neighbour]:\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(2))\n",
    "                    #self.structural_array.append(str(2)+'b')\n",
    "                    \n",
    "                else:\n",
    "                    x = bfs_4(self.graph_list, neighbour, node)\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(x+1))\n",
    "                    #self.structural_array.append(str(x+1)+'b')\n",
    "                \n",
    "            # otherwise when the neighbour's visit started before the current node's visit:\n",
    "            elif self.start_time[node] < self.start_time[neighbour]:\n",
    "                graph_list_copy = copy.deepcopy(self.graph_list)\n",
    "                graph_list_copy[node].remove(neighbour)\n",
    "                y = bfs_4(graph_list_copy, node, neighbour)\n",
    "                self.structural_array.append('forward ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(y-1))\n",
    "                #self.structural_array.extend((y-1)*['forward']) # -1 to exclude one edge: (A:B,C;B:C;C:[]) ...the dist A --> C is 2 without forward edge, but we are skipping only one activity\n",
    "                #self.structural_array.append(str(y-1)+'f')\n",
    "                \n",
    "            else:\n",
    "                #Possibly first check, whether two nodes connected by cross-type have identical parent\n",
    "                numberSkips, numberCross = common_ancestors(self.graph_list, node, neighbour)\n",
    "                #self.structural_array.append('forward ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(numberSkips))\n",
    "                self.structural_array.append('cross ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(numberCross))\n",
    "                #self.structural_array.append(str(numberSkips)+'f')\n",
    "                #self.structural_array.append(str(numberCross)+'c')\n",
    "    \n",
    "        # Indentation corrected:\n",
    "        self.end_time[node] = self.time\n",
    "        self.time += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3783713",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"int_strucLengthList3\"] = logVar.apply(lambda x: Graph2(x.int_adjList, x.indexList).dfs(), axis =1)\n",
    "#logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab386336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.960029069767442\n",
      "P@10: 0.9242798625792813\n",
      "Tri:  tensor(0.9026)\n",
      "Sil:  0.08512902120313036\n"
     ]
    }
   ],
   "source": [
    "#Jacc similarity based on edge types\n",
    "\n",
    "jacc_graph = matrix_calc(logVar[\"int_strucLengthList3\"],jaccard_similarity)\n",
    "results(jacc_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e122ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9571220930232558\n",
      "P@10: 0.898090644820296\n",
      "Tri:  tensor(0.8625)\n",
      "Sil:  0.15896867249184918\n"
     ]
    }
   ],
   "source": [
    "#Jacc sim based on edge types\n",
    "agg_jacc = Jacc1_dis + Jacc2_dis + jacc_graph\n",
    "results(agg_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67318150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308eff76",
   "metadata": {},
   "source": [
    "## Eventually Follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22dc753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial distance between strings\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def distanceSpatial(traceString, char1, char2):\n",
    "    positions_letter1 = [pos for pos, char in enumerate(traceString) if char == char1]\n",
    "    positions_letter2 = [pos for pos, char in enumerate(traceString) if char == char2]\n",
    "    \n",
    "    distList = []\n",
    "    \n",
    "\n",
    "    for i in range(len(positions_letter1)):\n",
    "        for j in range(len(positions_letter2)):\n",
    "            dist = positions_letter2[j] - positions_letter1[i]\n",
    "            if dist > 0:\n",
    "                    #print(dist)\n",
    "                distList.append(dist)\n",
    "                    \n",
    "    \n",
    "    if not distList: #distList.append(0) #in the case the char1 is after char2 asign dist 0, i.e. char2 cannot be reached from char1\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/min(distList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def commonDistance(trace1, trace2):\n",
    "    \n",
    "    commonSet = set(trace1) & set(trace2)\n",
    "\n",
    "    commonList = list(commonSet)\n",
    "    commonList.sort()\n",
    "    #print(commonList)\n",
    "\n",
    "    n = len(commonSet)\n",
    "    dist_matrix1 = np.zeros((n,n))\n",
    "    dist_matrix2 = np.zeros((n,n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix1[i,j] = distanceSpatial(trace1, commonList[i], commonList[j])\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix2[i,j] = distanceSpatial(trace2, commonList[i], commonList[j])\n",
    "    \n",
    "    #print(dist_matrix1, dist_matrix2)\n",
    "    return distance.cosine(dist_matrix1.ravel(), dist_matrix2.ravel())\n",
    "\n",
    "\n",
    "\n",
    "#x = 'ABCDEF'\n",
    "#y = 'ABCDEBCDEBCDEF'\n",
    "#z = 'ABCDEBCDEF'\n",
    "#print(dist_matrix)\n",
    "#distanceSpatial(x, 'A', 'E')\n",
    "#listVec = logVar[\"strings\"]\n",
    "#x= listVec[0]\n",
    "#y= listVec[1]\n",
    "#commonDistance(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e77fecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.46128435517970406\n",
      "P@10: 0.4549550739957716\n",
      "Tri:  tensor(0.8286)\n",
      "Sil:  0.06766086823284875\n"
     ]
    }
   ],
   "source": [
    "dist_matrix_evFollows = matrix_calc(logVar[\"strings\"],commonDistance)\n",
    "agg_evFollows = 0.7*dist_matrix_evFollows + 0.3*cos1_dis \n",
    "results(agg_evFollows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f9fa8",
   "metadata": {},
   "source": [
    "## Maximal Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82a82836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from suffix_tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3ba0fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a a', 'b', 'b d', 'c', 'c b', 'c d c', 'd', 'd b', 'd c', 'e']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree.maximal_repeats\n",
    "def maxRepeat(tree):\n",
    "    mrList=[]\n",
    "    for C, path in sorted(tree.maximal_repeats()):\n",
    "        mrList.append(str(path))\n",
    "    return mrList\n",
    "\n",
    "#test_tree = Tree({\"A\": \"aaacdcdcbedbccbadbdebdc\"})\n",
    "#maxRepeat(test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e6fa179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vector based on maximal repeats\n",
    "logVar[\"mrList\"] = logVar[\"strings\"].apply(lambda x: maxRepeat(Tree({\"A\": x})))\n",
    "logVar[\"mrVector\"] = logVar[\"mrList\"].apply(lambda x: createVector(tuple(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75d75b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9571220930232558\n",
      "P@10: 0.9028409090909092\n",
      "Tri:  tensor(0.8912)\n",
      "Sil:  0.18629615358356524\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on maxR\n",
    "\n",
    "cos_mr = matrix_calc(logVar[\"mrVector\"],cosineDist)\n",
    "results(cos_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3991e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.7854122621564482\n",
      "P@10: 0.6207650634249472\n",
      "Tri:  tensor(0.6617)\n",
      "Sil:  -0.06668386684998291\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on maxR\n",
    "\n",
    "euc_mr = matrix_calc(logVar[\"mrVector\"],euclidDist)\n",
    "results(euc_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f51a3b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9541490486257929\n",
      "P@10: 0.9013940274841438\n",
      "Tri:  tensor(0.8847)\n",
      "Sil:  0.1362689156049775\n"
     ]
    }
   ],
   "source": [
    "#Jaccard similarity based on maxR\n",
    "\n",
    "jacc_mr = matrix_calc(logVar[\"mrList\"],jaccard_similarity)\n",
    "results(jacc_mr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363e089",
   "metadata": {},
   "source": [
    "# Trace2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fa90ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R R R R R ± R R R R R R ± R ± R R R R ± ± ± ± ± ± R R R R R R R R R R R R R 4 R 2 2 2 O Q O H M J L ė ė ũ Ɯ Ɯ Ŭ ƙ Ů Ʊ Æ Æ Ë Ë É É Ă Ă Ă Ă Ă È Ę Ę Ƴ ƴ Ƹ Ũ Ũ Ů Ů Ů Ů Ů Ū Ū Ɯ ȹ ȁ ȁ ǻ ǿ ǿ ǿ ǿ Ȼ Ǿ Ǿ Ȼ ȃ ǽ ǽ Ȁ Ȁ ȫ Ȣ ɐ ɖ ˦ ˒ ʢ ʞ ʞ ʞ ʞ ʞ ʞ ʝ ʝ ʠ ʠ ˒ ˨ ˦ ˩ ͖ ̢ ̦ ̦ ͖ ͉ ͉ ͖ ͘ Ω Ͳ Ͳ Ͳ Ϭ Ϩ θ ϗ ϗ ϗ ϖ ϗ φ Ы Ы Ы а ѥ і з Ϭ ϩ χ φ π ϔ ͖ ͖ ͖ ̷ ̧ ̧ ̧ ̤ ͖ ͖ ̢ ̢ ͖ ̣ ̣ ̣ ̣ Ͳ ͯ Ͱ ͯ ͮ ͮ ͯ ͭ ͯ ͫ ͪ Ͳ ͩ ͨ ͬ ˤ ˢ ˒ ˋ ˒ ʢ ʝ ˬ ˬ ˬ ˬ ˒ ʟ ʟ ˒ ʡ ˒ ʠ ʠ ʠ ʞ ʢ ʞ ˨ ˨ Ǽ Ȍ Ȼ ȱ Ȼ Ȼ Ȼ ȁ ȁ ɓ ɒ ɣ ʐ ʐ ɥ ʐ ɖ ɖ ɖ Ƚ ɖ ʐ ɖ Ȁ Ȁ Ȁ Ȁ Ȁ ȹ ȹ Ƚ Ƚ ɘ ɘ Ȼ ǻ ȹ Ȱ Ȼ Ȼ ȅ ȏ ȁ ʐ ɖ ɥ ɖ Ȁ Ȁ ȯ ȯ Ⱥ Ⱥ Ȼ Ȼ Ȼ Ȼ Ⱥ ɘ ȹ Ȼ Ȁ Ȼ ȁ Ȼ ȁ ȁ ȁ Ȼ ɘ ɘ'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logVar[\"words\"] = logVar[\"c:n_chr\"].apply(lambda x: ' '.join(x))\n",
    "logVar[\"words\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b40b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(logVar['words'])\n",
    "\n",
    "# Step 1: Tokenize documents\n",
    "texts = [\n",
    "    [word for word in document.lower().split()]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "# Step 2: Tag the documents\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "tagged_docs = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7703182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive and evaluate model\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def trainModel(documents,vec_size):\n",
    "    \n",
    "    # Step 3: Initialize and train the model\n",
    "    model = Doc2Vec(vector_size=vec_size, window=3, min_count=0.0, min_alpha=0.025, epochs=10, dm = 0, alpha=0.025)\n",
    "    model.build_vocab(documents)\n",
    "    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    # Step 4: infer vectors for each document\n",
    "    vectors = [model.infer_vector(doc) for doc in texts]\n",
    "    \n",
    "    # Step 5: calculate cosine distances\n",
    "    cos = cosine_similarity(vectors)\n",
    "    \n",
    "    return results(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc288995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.0\n",
      "P@10: 0.0\n",
      "Tri:  tensor(0.0564)\n",
      "Sil:  0.24622111\n"
     ]
    }
   ],
   "source": [
    "trainModel(tagged_docs,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f23ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.0\n",
      "P@10: 0.0\n",
      "Tri:  tensor(0.0653)\n",
      "Sil:  0.23205745\n"
     ]
    }
   ],
   "source": [
    "trainModel(tagged_docs,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "424ac174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.0\n",
      "P@10: 0.0005681818181818183\n",
      "Tri:  tensor(0.0674)\n",
      "Sil:  0.21483004\n"
     ]
    }
   ],
   "source": [
    "trainModel(tagged_docs,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c68abf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.0\n",
      "P@10: 0.0005747885835095138\n",
      "Tri:  tensor(0.0710)\n",
      "Sil:  0.21265528\n"
     ]
    }
   ],
   "source": [
    "trainModel(tagged_docs,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309845fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
